[project]
name = "openfilter-mcp"
version = "0.1.0"
requires-python = ">=3.10,<3.13"
dependencies = [
    "code-context>=0.1.0",
    "fastmcp>=2.13.1",
    "gitpython>=3.1.45",
    "httpx>=0.28.1",
    "jq>=1.10.0",
]

[build-system]
requires = ["uv_build>=0.9.6,<0.10.0"]
build-backend = "uv_build"

[project.scripts]
index = "openfilter_mcp.preindex_repos:preindex_openfilter_repos"
serve = "openfilter_mcp.server:main"

[tool.uv]
config-settings-package = { "llama-cpp-python" = { "cmake.args" = "-DGGML_CUDA=on" } }

[[tool.uv.index]]
name = "llama-cpp-python-cuda"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124"
explicit = true

[[tool.uv.index]]
name = "llama-cpp-python-metal"
url = "https://abetlen.github.io/llama-cpp-python/whl/metal"
explicit = true

[[tool.uv.index]]
name = "llama-cpp-python-cpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cpu"
explicit = true

[tool.uv.sources]
code-context = { path = "./code-context" }
llama-cpp-python = [
    { git = "https://github.com/abetlen/llama-cpp-python", marker = "sys_platform == 'linux'" },
    { index = "llama-cpp-python-metal", marker = "sys_platform == 'darwin'" },
]

[dependency-groups]
dev = [
    "httpx>=0.28.1",
    "pytest>=9.0.2",
    "pytest-asyncio>=1.3.0",
    "pytest-httpx>=0.35.0",
]
